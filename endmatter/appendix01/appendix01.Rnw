\chapter{Computing with particles}
\label{app:computing-with-particles}

The particle filter is a computationally demanding process, so here we give some details about the more intensive components of the application.


\section{Resampling}
\label{app:particle-resampling}

Part of the particle filter update step is to, when required, resample the particles according to their weights. First, however, we will describe weighted resampling. Given a sample of size $N$, we use a \gls{rng} to obtain uniform numbers $u \in (0, 1)$. Since each particle has equal weight, we multiply $u$ by $N$ and round down (using the \emph{floor} function), so get
\begin{equation}
j = \lfloor uN \rfloor.
\end{equation}
Thus, we have sampled particle $j$.


When the particles are weighted, however, we cannot simply use $uN$. For example, here we have five particles, where each is represented by a rectangle whose width is equal to the particle's weight (the total weight is unity), as shown in \cref{fig:app_weighted_resampling}.
<<app_weighted_resampling,echo=FALSE,fig.width=9,fig.height=1,out.width="0.8\\textwidth",fig.align="center",fig.cap="Particle weights.",fig.pos="h">>=
set.seed(15)
N <- 5
w <- diff(c(0, sort(runif(N - 1)), 1))
wr <- cumsum(w)
wl <- c(0, wr[-N])
wc <- wl + (wr - wl) / 2
par(mar = c(0.1, 0.1, 0.1, 0.1))
plot.new()
plot.window(xlim = c(0, 1), ylim = c(-0.5, 1), xaxs = "i", yaxs = "i")
rect(wl + 0.005, 0.25, wr - 0.005, 0.75, col = "lightgray")
text(wc, 0, 1:N, cex = 1)
@

As before, we use a \gls{rng} to obtain a uniform random number $u$; to map the number to a particle, we compute \emph{cumulative weights} and find the maximum weight less than $u$:
\begin{equation}
j : \sum_{i=1}^j w\vi[j] \leq u.
\end{equation}



\section{Summary statistics}
\label{app:particle-summaries}

Another core part of the program is computing of summary statistics, for example the mean and variance. However, these are easily computed using \verb+std::accumulate+, for example, a vehicle's mean speed is obtained by:
\begin{lstlisting}
double mean = std::accumulate (
    x.begin (), x.end (),
    // initial value
    0.,
    // lambda function, where 'a' is the current value
    [](double a, Particle& p) {
        return a + p.get_weight () * p.get_speed ();
    }
);
\end{lstlisting}
Similarly, the variance can be calculated by passing a reference to the value of \verb+mean+ to the lambda function (within \verb+[]+), which becomes
\begin{lstlisting}
[&mean](double a, Particle& p) {
    return a + p.get_weight () * pow (p.get_speed () - mean, 2.);
}
\end{lstlisting}

More complicated, however, is estimation of \emph{quantiles}, since this requires sorting the vector of particles in order of the desired value. In the above example, to get the median speed, we need to first sort the particles by speed (slowest to fastest) and then sum their weights, in order, until the sum surpasses 0.5: the speed of the particle that does so is the median speed.
\begin{lstlisting}
std::sort (x.begin (), x.end (),
    (Particle& p1, Particle& p2)
    {
        return p1.get_speed () < p2.get_speed ();
    }
);
double wt = 0.;
int i = 0;
while (wt < 0.5) wt += x.at (i++).get_weight ();
double median = x.at (i).get_speed ();
\end{lstlisting}

The issue here is the first step, sorting, which has computational complexity of $\mathcal{O}(N\log N)$\footnote{as described by the documentation page for \verb+std::sort+, \url{https://en.cppreference.com/w/cpp/algorithm/sort}}. If we wish to estimate the median distance and the median speed, we need to sort the particles \emph{twice}. In \cref{cha:etas}, we used multiple quantiles for each stop: this requires sorting each vehicle's particles once for each upcoming stop. However, since we use a subsample to compute \glspl{eta} ($N^\star$), the complexity is significantly smaller, $\mathcal{O}(N^\star\log N^\star)$; if we attempted to use all $N$ particles (in the simulation used in \cref{cha:prediction}, we used $N=10000$) the application would have taken far longer.
