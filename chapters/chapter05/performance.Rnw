\section{Real-time performance}
\label{sec:prediction_performance}

The real-time performance of the our application has always been the main bottleneck, in that it needs to run in real-time and provide arrival time predictions as soon as possible after the data is received.Â During the simulation run to obtain the results from \cref{sec:prediction_model_comparison}, we also recorded the timings of individual parts of the program, of which overall averages are displayed in \cref{tab:prediction_timing}. In it, we have two timers: wall clock and CPU clock. Wall clock is the real-world time passed, while the CPU clock is the time spent on the processor. Since we are using only one core, these values are approximately similar, with the exception of the ``Load data'' step, which involves calling the \gls{gtfs} \gls{api} and waiting for the data to download. The wall time can be reduced in the particle filter and \gls{eta} prediction steps by using more than a single core: using two cores would halve the time as the CPU time is spread over multiple cores.


On average, the program takes less than 15~seconds, which is twice as fast as our initial target of 30~seconds. The most intensive component is updating of vehicle states, which involves updating 10,000 particles for each operating vehicle. This is followed by the \gls{eta} prediction step, which involves far fewer particles (we used 200 per trip), but each particle is required to estimate more per iteration [better wording]. However, the main advantage is that, in the vehicle update step, we perform a full weighted resample of the particles, which involves a full copy of all $\Np$ particles, plus sorting (if applicable). In the \gls{eta} step, we are able to use a single pointer to iterate over sampled particles, which completely avoids any copying.

<<prediction_timing,echo=FALSE,message=FALSE,warning=FALSE,cache=F>>=
suppressPackageStartupMessages({
    library(tidyverse)
    library(knitr)
    library(kableExtra)
})
options(scipen = 10)

timings <- read_csv("data/timings.csv") %>% filter(iteration > 1)

tbl_times <- timings %>%
    group_by(what) %>%
    summarize(
        n = n(),
        wall_mean = mean(wall),
        wall_se = sd(wall) / sqrt(n),
        cpu_mean = mean(cpu),
        cpu_se = sd(cpu) / sqrt(n)
    ) %>%
    select(-n) %>%
    bind_rows(
    timings %>%
        group_by(iteration) %>%
        summarize(
            timestamp = first(timestamp),
            wall = sum(wall),
            cpu = sum(cpu)
        ) %>%
        ungroup() %>%
        summarize(
            what = "Total",
            n = n(),
            wall_mean = mean(wall),
            wall_se = sd(wall) / sqrt(n),
            cpu_mean = mean(cpu),
            cpu_se = sd(cpu) / sqrt(n)
        ) %>%
        select(-n)
    )


tbl_times <- tbl_times[c(1, 4, 5, 3, 2, 6, 7), ]
tbl_times <- tbl_times %>%
    mutate(
        wall_mean = format(signif(wall_mean, 3), drop0trailing = TRUE),
        wall_se = paste0("(", signif(wall_se, 2), ")"),
        cpu_mean = format(signif(cpu_mean, 3), drop0trailing = TRUE),
        cpu_se = paste0("(", signif(cpu_se, 2), ")")
    )
tbl_times$what <- c(
    "(L) Load data",
    "(U) Update vehicle information",
    "(V) Vehicle state update",
    "(N) Network state update",
    "(P) Predict ETAs",
    "(W) Write ETAs to protobuf feed",
    "(T) Total iteration time"
)
names(tbl_times) <- c("", "Wall clock", "(SE)", "CPU time", "(SE)")
kable(
    tbl_times,
    align = "lrlrl",
    booktabs = TRUE,
    caption = "Time taken during various parts of the program, running on a single core."
) %>%
    row_spec(6, extra_latex_after = "\\midrule")
@


However, there is a high level of variability in the number of buses operating at any given time (figure X), so in \cref{fig:prediction_timing_time} we have displayed the timings for each iteration over the day. We again see the peak hour effect, where there are upwards of 1000~vehicles operating. This increase pushes the total iteration time to 30~seconds. However, the two most intensive steps (vehicle update and \gls{eta} prediction) are thread-safe\footnote{At least, they are supposed to be\ldots currently a segfault occurring when running on multiple cores, not sure why.} and implemented to run on multiple cores, if available, which can significantly speed up iteration timings.


<<prediction_timing_time,warning=FALSE,echo=FALSE,cache=F,dependson=c(-1),fig.width=9,fig.height=4,out.width="\\textwidth",fig.cap="Timing results over time for various stages of the program: (L) Load data, (O) Update vehicles information, (V) Vehicle state update, (N) Network state update, (P) Predict ETAs, (W) Write ETAs to protobuf feed, (T) Total iteration time.">>=
# totals:
total_time <- timings %>%
    group_by(timestamp) %>%
    summarize(wall = sum(wall), cpu = sum(cpu), what = "total")
timings %>%
    bind_rows(total_time) %>%
    mutate(
        timestamp = as.POSIXct(timestamp, origin = "1970-01-01"),
        Stage = factor(what,
            levels = sort(unique(what))[c(1, 5, 6, 4, 2, 7, 3)],
            labels = c("L", "U", "V", "N", "P", "W", "T")
        )
    ) %>%
    arrange(timestamp) %>%
    ggplot(aes(timestamp)) +
        geom_path(aes(y = wall, colour = Stage, group = Stage)) +
        theme_classic() +
        xlab("Time") + ylab("Iteration timing (seconds, wall clock)")
@
