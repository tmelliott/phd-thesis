\subsection{Normal approximation}
\label{sec:prediction_arrival_time_normal}

Due to the computational demand of the particle filter, significant speed improvements can be obtained if we were to use a normal approximation instead. The network state is already represented as multivariate normal random variable, which we can easily apply transformations to sum over the dimensions. The issues lies with stop dwell times, which have a point mass at zero, leading to a mixture predictive distribution: for each stop the vehicle must pass, there are twice as many components, so after $m$ stops there will be $2^m$ components. However, these mostly converge after a few stops as shown in \cref{fig:normal_approx}.



<<normal_approx,cache=TRUE,echo=FALSE,fig.width=9,fig.height=3,fig.subcap=c("One intermediate stop", "Two intermediate stops", "Three intermediate stops", "Five intermediate stops"),fig.ncol=1,fig.colsep=rep("\\\\",5),fig.cap="Normal approximation">>=
napprox <- function(n, pi = rep(0.8, n), t = c(80, 15), mu = rep(20, n), sigma = rep(5, n)) {
    # indicator of stopping:
    d <- do.call(expand.grid, lapply(1:n, function(i) 0:1))
    pi <- apply(sapply(1:n, function(i) ifelse(d[,i], pi[i], 1 - pi[i])), 1, prod)
    mu <- apply(sapply(1:n, function(i) t[1] + ifelse(d[,i], mu[i], 0)), 1, sum)
    sigma <- sqrt(apply(sapply(1:n, function(i) t[2] + ifelse(d[,i], sigma[i]^2, 0)), 1, sum))

    z <- apply(rmultinom(1e6, 1, pi), 2, function(x) which(x == 1))
    x <- rnorm(1e6, mu[z], sigma[z])
    hist(x, 50, freq = FALSE, col = "gray90", lty = 0)
    for (i in 1:nrow(d))
        curve(dnorm(x, mu[i], sigma[i]) * pi[i], n = 1001, add = TRUE,
            lty = 2, col = "gray20")


    ## Can we calculate the quantiles?
    q <- quantile(x, prob = c(0.05, 0.5, 0.95))
    abline(v = q, col = "orangered", lwd = 2)

    f <- function(x, q = 0.95) {
        (sum(pi * pnorm(x, mu, sigma)) - q)^2
    }
    qq <- sapply(c(0.05, 0.5, 0.95), function(q) optimize(f, range(x), q = q)$minimum)
    abline(v = qq, col = "blue", lty = 2)

    ## mean and var
    mean <- sum(pi * mu)

    EVarX <- sum(pi * sigma^2)
    VarEX <- sum(pi * mu^2) - sum(pi * mu)^2
    sd <- sqrt(EVarX + VarEX)

    curve(dnorm(x, mean, sd), n = 1001, add = TRUE, col = "orangered")

    ## Compare quantiles of single normal vs mixture
    qx <- qnorm(c(0.05, 0.5, 0.95), mean, sd)
    abline(v = qx, col = "green4")
    invisible(list(
        x = x,
        pi = pi, mu = mu, sigma = sigma,
        error = sum((qx - qq)^2)
    ))
}
napprox(1, pi = 0.8, mu = 30, sigma = 20, t = c(30, 5))
napprox(2, pi = c(0.8, 0.5), mu = c(30, 20), sigma = c(20, 10), t = c(70, 15))
napprox(3, pi = c(0.5, 0.5, 0.5), mu = c(30, 20, 40), sigma = c(20, 10, 20), t = c(120, 20))
napprox(5, pi = c(0.5, 0.5, 0.5, 0.5, 0.3), mu = c(30, 20, 40, 20, 6), sigma = c(20, 10, 20, 10, 5), t = c(200, 40))
@


In order to use a mixture of normals to encorporate stopping uncertainty, we express the mean and uncertainty as vectors $\tilde\mu$ and $\tilde\sigma^2$, respectively, along with a third vector $\tilde\pi$ denoting the $\tilde N$ mixture weights\footnote{We are using the tilde over parameters, e.g., $\tilde x$, to help distinguish them from others used throughout the thesis}, such that
\begin{equation}
\label{eq:ch5:mixture_weight_spec}
\tilde\pi_i > 0, i = 1, \ldots, \tilde N
\text{ and } \sum_{i=1}^{\tilde N} \tilde\pi_i = 1.
\end{equation}
The arrival time at stop $j + n$ is given by
\begin{equation}
\label{eq:arrival_time_normal_approx}
\Tarr_{j+n} | \tilde\mu, \tilde\sigma^2, \tilde\pi, \RouteNWstate =
\sum_{\ell=j}^{j+n-1} \RouteNWstateseg_\ell +
\sum_{i=1}^{\tilde N} \tilde\pi_i z_i,\quad
z_i \sim \Normal{\tilde\mu_i}{\tilde\sigma^2_i}.
\end{equation}


To compute each component, each component $i$ has an indicator of whether or not it stopped at stop $m$, $I_{im} = \{0,1\}$. Then
\begin{equation}
\label{eq:mixture_dwell_times}
\begin{split}
\tilde\mu_i &= \sum_{m=j}^{j+n} I_{im} \dwell_m \\
\tilde\sigma_i^2 &= \sum_{m=j}^{j+n} I_{im} \dwellvar_m
\end{split}
\end{equation}
since we are assuming dwell times at individual stops are independent of each other.


The vector of indicators is simply a branching tree for each stop, all current components are duplicated: one is assigned $I_{i,m+1} = 1$ and the other 0.


Finally, mixture weights are obtained through the stopping probability at each stop, $\pi_j$:
\begin{equation}
\label{eq:ch5:mixture_weights}
\begin{split}
\tilde\pi_i &= \prod_{m=j}^{j+n} \tilde p_{im} \\
\tilde p_{im} &=
\begin{cases}
\pi_m & \text{if } I_{im} = 1 \\
1 - \pi_m & \text{otherwise.}
\end{cases}
\end{split}
\end{equation}


This all works okay for predicting a few stops ahead, but after some time the mixture weights become small and the components combine into more-or-less a single one, as shown in \cref{fig:normal_approx}. To prevent $\tilde N$ from becoming too large, the full distribution is simplified into a single component with mean and variance those of the mixture. These can be computed by
\begin{equation}
\label{eq:mixture_mean}
\begin{split}
\E{\Tarr_m | \tilde\pi, \tilde\mu, \tilde\sigma^2, \RouteNWstate} &=
\E{\sum_{\ell=j}^{j+n-1} \RouteNWstateseg_\ell +
  \sum_{i=1}^{\tilde N} \tilde\pi_i z_i}
= \sum_{\ell=j}^{j+n-1} \E{\RouteNWstateseg_\ell} +
  \sum_{i=1}^{\tilde N} \tilde\pi_i \E{z_i} \\
&= \sum_{\ell=j}^{j+n-1} \hat\RouteNWstateseg_\ell +
  \sum_{i=1}^{\tilde N} \tilde\pi_i \tilde\mu_i
\end{split}
\end{equation}
and
\begin{equation}
\label{eq:mixture_variance}
\begin{split}
\Var{\Tarr_m | \tilde\pi, \tilde\mu, \tilde\sigma^2, \RouteNWstate} &=
\Var{\sum_{\ell=j}^{j+n-1} \RouteNWstateseg_\ell +
  \sum_{i=1}^{\tilde N} \tilde\pi_i z_i}
= \sum_{\ell=j}^{j+n-1} \Var{\RouteNWstateseg_\ell} +
  \sum_{i=1}^{\tilde N} \tilde\pi_i^2 \Var{z_i} \\
&= \sum_{\ell=j}^{j+n-1} \hat\RouteNWstatesegvar_\ell +
  \sum_{i=1}^{\tilde N} \tilde\pi_i \tilde\sigma_i^2
\end{split}
\end{equation}
by assuming segment travel time and dwell time are independent---assuming otherwise makes this model impossible to work with; indeed, this model versus the particle filter (which makes no such assumption) is effectively testing the viability of this assumption.


For quantiles $q_\alpha$ we need to use an optimisation algorithm to solve
\begin{equation}
\label{eq:mixture_quadratic}
\left[
  p\left(\Tarr_m | \tilde\pi, \tilde\mu, \tilde\sigma^2, \RouteNWstate\right) - q_\alpha
\right]^2 = 0
\end{equation}
which is straightforward using Brent's Algorithm \citep{Brent_1973}, which is implemented in the Boost C++ library \citep{cn}.


However, after about eight stops, the number of components will be in excess of $2^8 = \Sexpr{2^8}$, after which point it would be more efficient to just use the particle filter. Instead, we use a simplification criteria such that, when passed, a single new component is close enough in approximation to the set of components that it can replace them. We have chosen to compare quantiles, and if the maximum absolute difference is less than one minute\footnote{When providing \glspl{eta} to commuters, they are generally rounded to integer minutes anyway}, we replace the set of components with a single new one with mean and variance as specified in \cref{eq:mixture_mean,eq:mixture_variance}. Subsequent stops will begin the process anew, and iteratively check if a single distribution is an adequate approximation.


We also need to prevent the mixture from becoming too large: if $\tilde N > 8$, we combine components with $\tilde\pi_i < \frac{1}{2}\max_i(\pi_i)$, using appropriately modified versions of \cref{eq:mixture_mean,eq:mixture_variance}. In practice, after about five stops the distribution is approximately normal, and the error is minimal. \textcolor{red}{a figure for this}
