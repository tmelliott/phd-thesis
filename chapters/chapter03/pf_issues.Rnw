\subsection{Real-time performance of the \pf{}}
\label{sec:pf_issues}



The two components of the model to assess are the iteration timings and the performance of the particle filter itself. That is, does the program run fast enough to be feasible in real-time, and is the model (and its \pf{} implementation) capable of modelling transit vehicles in real-time?


\paragraph{Is the particle filter fast enough?}
At peak hour on a typical weekday morning, there can be in excess of 1000~buses operating in Auckland. This leads to having more than $1000\Np$~particles in memory, each being mutated and reweighted approximately once every 30~seconds or so. By varying $N$, we can control how quickly each set of observations is processed. \Cref{fig:pf_timings} shows the average timings of the vehicle model component of our application, as well as the average time per particle for varying $N$. More particles require more processing power, though there is additional overhead during the \emph{resampling} phase. Limiting the frequency of resampling is therefore necessary to make the program run faster, which is why we use the effective sample size, $\Neff$, described in \cref{sec:pf} on \cpageref{eq:Neff}.


<<pf_timings,echo=FALSE,message=FALSE,cache=TRUE,fig.width=6,fig.height=3,out.width=".8\\textwidth",fig.align="center",fig.cap="Timings.">>=
suppressPackageStartupMessages(library(tidyverse))

times <- do.call(
    bind_rows,
    lapply(
        list.files("~/Documents/uni/transitr/simulations/oldsim2",
            pattern = "sim_",
            full = TRUE
        ),
        function(dir) {
            if (!file.exists(file.path(dir, "timings.csv"))) return(NULL)
            sim <- basename(dir)
            siminfo <- strsplit(sim, "_")[[1]][-1]
            if (grepl("e", siminfo[3])) siminfo[3] <- format(as.numeric(siminfo[3]), scientific = FALSE)
            siminfo <- as.numeric(gsub("-", ".", siminfo))
            read_csv(file.path(dir, "timings.csv")) %>%
                mutate(
                    sim = sim,
                    n_particles = siminfo[1],
                    gps_error = siminfo[2],
                    system_noise = siminfo[3],
                    timestamp = as.POSIXct(timestamp, origin = "1970-01-01")
                )
        }
    )
)
date <- format(times$timestamp[1], "%Y-%m-%d")
trange <- as.POSIXct(paste(date, c("13:30", "14:00")))

tsmry <- times %>%
    filter(
        what == "updating vehicle states" &
        timestamp >= trange[1] & timestamp <= trange[2]
    ) %>%
    group_by(sim) %>%
    summarize(
        cpu = mean(cpu),
        wall = mean(wall),
        n_particles = first(n_particles),
        gps_error = first(gps_error),
        system_noise = first(system_noise)
    ) %>%
    ungroup()

p0 <- tsmry %>%
    group_by(n_particles) %>%
    summarize(wall = mean(wall)) %>%
    ggplot(aes(y = n_particles)) +
        theme_classic() +
        scale_y_continuous("Number of particles")

p1 <- p0 +
    geom_segment(
        aes(x = wall, xend = 0, yend = n_particles)
    ) +
    geom_point(aes(wall)) +
    scale_x_continuous("Average iteration time (ms)")

p2 <- p0 +
    geom_segment(
        aes(x = wall / n_particles, xend = 0, yend = n_particles)
    ) +
    geom_point(aes(wall / n_particles)) +
    scale_x_continuous("Time per particle (ms)")

library(patchwork)
p1 + p2
@


\paragraph{How does the model perform?}


<<model_performance_prep,echo=FALSE,message=FALSE,cache=TRUE>>=

suppressPackageStartupMessages(library(tidyverse))

get_sim_files <- function(sim) {
    simfile <- file.path(
        "simdata",
        sim,
        "modeleval.rds"
    )
    if (file.exists(simfile)) {
        return(readRDS(simfile))
    }
    zipfile <- "~/Documents/uni/transitr/simulations/oldsim2.zip"
    x <- try({
        siminfo <- strsplit(sim, "_")[[1]][-1]
        if (grepl("e", siminfo[3]))
            siminfo[3] <- format(as.numeric(siminfo[3]), scientific = FALSE)
        siminfo <- as.numeric(gsub("-", ".", siminfo))

        zfiles <- unzip(zipfile, list = TRUE)
        zfiles <- zfiles[
            grepl(sim, zfiles$Name) &
            grepl("modeleval/vehicle_[A-Z0-9]+\\.csv", zfiles$Name),
            ]

        library(parallel)
        cl <- makeCluster(4L)
        on.exit(stopCluster(cl))
        clusterExport(cl, c("zipfile"))
        clusterEvalQ(cl, library(magrittr))
        do.call(bind_rows,
            pbapply::pblapply(sample(zfiles$Name),
                function(x) {
                    print(x)
                    tf <- unzip(zipfile, files = x)
                    on.exit(unlink(tf))
                    readr::read_csv(tf,
                        col_names = c(
                            "vehicle_id", "trip_id", "ts", "prior_mse",
                            "posterior_mse", #"sumwt", "varwt",
                            "post_speed", "prior_speed_var",
                            "posterior_speed_var", "dist_to_path",
                            "Neff", "resample", "n_resample", "bad_sample"
                        ),
                        col_types = "ccidddddddiii",
                        progress = FALSE
                    ) %>%
                        dplyr::mutate(
                            ts = as.POSIXct(ts, origin = "1970-01-01")
                        )
                },
                cl = cl
            )
        ) %>% mutate(
            sim = sim,
            n_particles = siminfo[1],
            gps_error = siminfo[2],
            system_noise = siminfo[3]
        )
    }, silent = TRUE)
    if (inherits(x, "try-error")) return(NULL)
    dir.create(dirname(simfile), recursive = TRUE)
    saveRDS(x, simfile)
    x
}

zipfile <- "~/Documents/uni/transitr/simulations/oldsim2.zip"
zfiles <- unzip(zipfile, list = TRUE)
sims <- unique(sapply(str_split(zfiles$Name, "/"), function(p) p[2]))
sims <- sims[grepl("^sim_", sims)]
res <- do.call(bind_rows, pbapply::pblapply(sims, get_sim_files))

sim_results <- res %>% filter(dist_to_path < 20) %>%
    group_by(n_particles, gps_error, system_noise) %>%
    summarize(
        Neff = mean(Neff, na.rm = TRUE),
        p_bad = mean(bad_sample),
        n = n()
    )

@


To assess how well our model performs in real-time, we repeated the simulation with a range of values of system noise $\Vnoise$, \gls{gps} error $\GPSerr$, and the number of particles $\Np$. For each simulation, we computed \emph{proportional effective sample size}, \emph{degeneration rate}, and \emph{relative variance}.

The \emph{proportional effective sample size} is the effective sample size relative to $N$, $\tilde N_\text{eff} = \frac{\Neff}{\Np}$. The higher this value, the less often the vehicle's state needs resampling, decreasing the average iteration time. \Cref{fig:model_performance_neff} shows the effect of $\Np$, system noise, and \gls{gps} error on $\tilde N_\text{eff}$. The most striking relationship is between $\tilde N_\text{eff}$ and \gls{gps} error: for larger error, more particles retain a high likelihood, and so the total weight is more evenly distributed. Conversely, larger values of system noise result in more variation between particles, leading to fewer particles ending near the observation position and decreasing $\tilde N_\text{eff}$.

<<model_performance_neff,echo=FALSE,message=FALSE,cache=TRUE,dependson="model_performance_prep",fig.width="8",fig.height=2.5,out.width="\\textwidth",fig.align="center",fig.cap="Proportional effective sample size for varying values of GPS error, system noise, and number of particles.">>=

ggplot(sim_results,
    aes(gps_error, Neff / n_particles,
        group = system_noise,
        colour = as.factor(system_noise)
    )
) +
    geom_path() +
    geom_point() +
    facet_grid(~n_particles,
        labeller = as_labeller(function(x) paste("N =", x))) +
    labs(colour = "System noise") +
    theme_classic() +
    theme(strip.background = element_blank()) +
    xlab("GPS error (m)") +
    ylab("Proportional effective sample size")
    #ylab(expression(bar(N)[eff]/N))

@


The \emph{degeneration rate} is the proportion of samples in which no particles end near the vehicle's reported position. In this case, all the particle likelihoods tend to zero, so the weights become undefined, resulting in the need to reinitialise the vehicle's state which results in the loss of any vehicle speed information along the most recently travelled road segment(s). We see from \cref{fig:model_performance_degen} that increasing \gls{gps} error or the number of particles reduces degeneration rate while increasing system noise shows a negligible reduction in degeneration rate. Larger \gls{gps} error means that particles do not need to end as close to the observed location to have a positive likelihood while increasing $\Np$ means more chance for a particle to end near the true bus.

<<model_performance_degen,echo=FALSE,message=FALSE,cache=TRUE,dependson="model_performance_prep",fig.width="8",fig.height=2.5,out.width="\\textwidth",fig.align="center",fig.cap="Degeneration rate for varying values of GPS error, system noise, and number of particles.">>=

ggplot(sim_results,
    aes(
        gps_error, p_bad,
        group = system_noise,
        colour = as.factor(system_noise)
    )
) +
    geom_path() +
    geom_point() +
    facet_grid(~n_particles,
        labeller = as_labeller(function(x) paste("N =", x))) +
    labs(colour = "System noise") +
    theme(legend.position = "none") +
    theme_classic() +
    theme(strip.background = element_blank()) +
    xlab("GPS error (m)") +
    ylab("Degeneration rate")

@

Finally, we have \emph{relative speed variance} along roads, which we use to compare the precision of the various models. It is the ratio of variance for a single simulation compared to the overall variance for all simulations. Let $\vec z_\ell^{e,s}$ be a vector of all vehicle speeds along road segment $\ell$ during the simulation with \gls{gps} error and system noise equal to $e$ and $s$, respectively. The ratio of the variance of speed for the single simulation compared to all simulations along one single road segment is given by
\begin{equation}
\label{eq:rel_speed_var_ratio}
v_\ell^{e,s} =
\frac{
    \mathrm{Var}(\vec z_\ell^{e,s})
}{
    \mathrm{Var}(\cup_e\cup_s \vec z_\ell^{e,s})
}.
\end{equation}
That is, for each segment in each simulation, we have a value representing whether this simulation estimates speed more or less accurately. We then compute the average ratio for all $L$ road segments,
\begin{equation}
\label{eq:rel_speed_var}
\bar v^{e,s} = \frac{1}{L} \sum_{\ell=1}^L v_\ell^{e,s}.
\end{equation}
A small value of $\bar v^{e,s}$ tells us that, on average, the simulation with \gls{gps} error $e$ and system noise $s$ estimates road speed \emph{with greater precision} than the other simulations. \Cref{fig:model_performance_var} presents these results, where we see the greatest effect on relative variance caused by increasing \gls{gps} error. Increasing $\Np$ produces a small decrease, and changes to system noise show no discernible effect.


<<model_performance_var,echo=FALSE,message=FALSE,cache=TRUE,dependson="model_performance_prep",fig.width="8",fig.height=2.5,out.width="\\textwidth",fig.align="center",fig.cap="Relative speed variance for varying values of GPS error, system noise, and number of particles.">>=

if (file.exists("simdata/nw_summary.rda")) {
    load("simdata/nw_summary.rda")
} else {
    if (!file.exists("simdata/nwtimes.rda")) {
        stop("Please copy nwtimes.rda from ANZJS paper")
    }
    load("simdata/nwtimes.rda")
    segids <- table(nwtimes$segment_id) %>% sort %>% tail(10) %>% names
    sdata <- nwtimes %>%
        group_by(n_particles, gps_error, system_noise) %>%
        do(
            (.) %>% filter(timestamp > min((.)$timestamp))
        ) %>%
        ungroup %>%
        group_by(segment_id) %>%
        do(
            (.) %>% mutate(
                tt.sd = sd(mean, na.rm = TRUE)
            )
        ) %>%
        ungroup %>%
        group_by(n_particles, gps_error, system_noise, segment_id) %>%
        summarize(
            varp = sd(mean, na.rm = TRUE) / first(tt.sd)
        ) %>%
        ungroup %>%
        group_by(n_particles, gps_error, system_noise) %>%
        summarize(
            varp.mean = mean(varp, na.rm = TRUE),
            varp.sd = sd(varp, na.rm = TRUE) / sqrt(n())
        )
    save(sdata, file = "simdata/nw_summary.rda")
}

ggplot(sdata,
    aes(
        gps_error, varp.mean,
        group = system_noise,
        colour = as.factor(system_noise)
    )
) +
    geom_linerange(
        aes(
            ymin = varp.mean - 2 * varp.sd,
            ymax = varp.mean + 2 * varp.sd
        )
    ) +
    geom_point() +
    geom_path(
        aes(

        )
    ) +
    facet_grid(~n_particles,
        labeller = as_labeller(function(x) paste("N =", x))) +
    labs(colour = "System noise") +
    theme_classic() +
    theme(strip.background = element_blank()) +
    xlab("GPS error (m)") +
    ylab("Relative variance")

@


The results displayed in \cref{fig:model_performance_neff,fig:model_performance_degen,fig:model_performance_var} present a \emph{trade-off} between performance and estimation. Increasing \gls{gps} error increases the effective sample size, which reduces the frequency of resampling and thus speeding up each iteration. We also see a reduction in the rate of degeneration, which implies the particle filter is less likely to lose the vehicle and provide the desired speed estimates. However, increasing \gls{gps} error also increases the relative uncertainty of speed estimates. Increasing the number of particles generally results in a reduction in both degeneration rate and relative uncertainty, but, from \cref{fig:pf_timings}, this comes at the cost of increased computational demand.
